{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Pr\u00e1ctica # 8 (Compilaci\u00f3n)\n",
    "\n",
    "En esta clase estaremos implementando un **int\u00e9rprete de expresiones regulares**. Utilizaremos aut\u00f3matas finitos como mecanismo reconocedor del lenguaje que denota cada expresi\u00f3n regular. Nos apoyaremos en las operaciones entre aut\u00f3matas implementadas en las clases anteriores para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.automata import NFA, DFA, nfa_to_dfa\n",
    "from cmp.tools.automata import automata_union, automata_concatenation, automata_closure, automata_minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar una expresi\u00f3n de determinado dominio no deber\u00eda parecernos un problema salido de la nada. Desde comienzos del curso, y hasta unas clases atr\u00e1s, estuvimos enfrent\u00e1ndonos al problema de evaluar expresiones aritm\u00e9ticas. Ahora nos enfrentamos a un problema similar, cambiando sumas por uniones, productos por concatenaciones, etc. Esto implica que la metodolog\u00eda que usaremos es la misma: obtendremos una **representaci\u00f3n sem\u00e1ntica**, **parseando** seg\u00fan una **gram\u00e1tica** del lenguaje de expresiones, cuyos s\u00edmbolos son los **tokens** que obtenemos del **lexer**.\n",
    "\n",
    "Curiosamente, llegamos este punto con el objetivo de implementar el lexer que alimente al parser durante la contrucci\u00f3n del compilador. Ahora nos apoyaremos en todo lo implementado del parser para construir el lexer. Claramente, el lexer (_tokenizer_) que usaremos para construir el generador de lexer ser\u00e1 un versi\u00f3n b\u00e1sica, a _pico y pala_, pues los tokens de las expresiones regulares son muy f\u00e1ciles de extraer.\n",
    "\n",
    "### Nodos del AST\n",
    "\n",
    "Pasemos a definir los nodos del AST. Usaremos como base las clases `Node`, `AtomicNode`, `UnaryNode` y `BinaryNode` para mantener la compatibilidad con el `printer` de AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class AtomicNode(Node):\n",
    "    def __init__(self, lex):\n",
    "        self.lex = lex\n",
    "\n",
    "class UnaryNode(Node):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        \n",
    "    def evaluate(self):\n",
    "        value = self.node.evaluate() \n",
    "        return self.operate(value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class BinaryNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def evaluate(self):\n",
    "        lvalue = self.left.evaluate() \n",
    "        rvalue = self.right.evaluate()\n",
    "        return self.operate(lvalue, rvalue)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "from cmp.ast import get_printer\n",
    "printer = get_printer(AtomicNode=AtomicNode, UnaryNode=UnaryNode, BinaryNode=BinaryNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El m\u00e9todo `evaluate` debe compilar la expresi\u00f3n regular. Dicho de otra forma, debe devolver el `NFA` que reconoce el lenguaje denotado por la expresi\u00f3n regular en cuesti\u00f3n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = '\u03b5'\n",
    "\n",
    "class EpsilonNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "EpsilonNode(EPSILON).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        s = self.lex\n",
    "        # Your code here!!!\n",
    "\n",
    "SymbolNode('a').evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClosureNode(UnaryNode):\n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "    \n",
    "ClosureNode(SymbolNode('a')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "UnionNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "ConcatNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram\u00e1tica\n",
    "\n",
    "Habiendo definido los nodos del AST, pasemos a dise\u00f1ar la gram\u00e1tica atributada para construirlo. Recordemos que es necesario que la gram\u00e1tica no sea ambigua (para ser parseable), no tener prefijos comunes ni recursividad izquierda (para ser parseable con un parser LL(1)) y que respete la prioridad de los operadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Grammar\n",
    "\n",
    "G = Grammar()\n",
    "\n",
    "E = G.NonTerminal('E', True)\n",
    "T, F, A, X, Y, Z = G.NonTerminals('T F A X Y Z')\n",
    "pipe, star, opar, cpar, symbol, epsilon = G.Terminals('| * ( ) symbol \u03b5')\n",
    "\n",
    "# > PRODUCTIONS???\n",
    "# Your code here!!!\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Para el lexer a _pico y pala_, procederemos como de costumbre: mantendremos una colecci\u00f3n con los tokens de lexema fijo y cualquier otro elemento ser\u00e1 tratado como s\u00edmbolo. Los lexemas no se obtendr\u00e1n de separar por espacios, sino de considerar cada caracter de la cadena original como lexema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "def regex_tokenizer(text, G, skip_whitespaces=True):\n",
    "    tokens = []\n",
    "    # > fixed_tokens = ???\n",
    "    # Your code here!!!\n",
    "    for char in text:\n",
    "        if skip_whitespaces and char.isspace():\n",
    "            continue\n",
    "        # Your code here!!!\n",
    "        \n",
    "    tokens.append(Token('$', G.EOF))\n",
    "    return tokens\n",
    "\n",
    "tokens = regex_tokenizer('a*(a|b)*cd | \u03b5',G)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Usaremos un parser LL(1) para obtener un parse izquierdo de la cadena (expresi\u00f3n regular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.parsing import metodo_predictivo_no_recursivo\n",
    "\n",
    "parser = metodo_predictivo_no_recursivo(G)\n",
    "left_parse = parser(tokens)\n",
    "left_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST\n",
    "\n",
    "Para obtener el AST evaluaremos los atributos de la gram\u00e1tica. Esto devolver\u00e1 el AST sintetizado en la producci\u00f3n ra\u00edz de la gram\u00e1tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.evaluation import evaluate_parse\n",
    "\n",
    "ast = evaluate_parse(left_parse, tokens)\n",
    "print(printer(ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aut\u00f3mata\n",
    "\n",
    "Y para obtener el aut\u00f3mata simplemente invocamos el m\u00e9todo `evaluate` de la ra\u00edz del AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfa = ast.evaluate()\n",
    "nfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convirt\u00e1moslo ahora en DFA para comprobar que reconoce las cadenas del lenguaje denotado por la expresi\u00f3n regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = nfa_to_dfa(nfa)\n",
    "display(dfa)\n",
    "\n",
    "assert dfa.recognize('')\n",
    "assert dfa.recognize('cd')\n",
    "assert dfa.recognize('aaaaacd')\n",
    "assert dfa.recognize('bbbbbcd')\n",
    "assert dfa.recognize('bbabababcd')\n",
    "assert dfa.recognize('aaabbabababcd')\n",
    "\n",
    "assert not dfa.recognize('cda')\n",
    "assert not dfa.recognize('aaaaa')\n",
    "assert not dfa.recognize('bbbbb')\n",
    "assert not dfa.recognize('ababba')\n",
    "assert not dfa.recognize('cdbaba')\n",
    "assert not dfa.recognize('cababad')\n",
    "assert not dfa.recognize('bababacc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, aplicaremos el algoritmo de minimizaci\u00f3n de aut\u00f3matas para obtener una versi\u00f3n m\u00e1s compacta del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = automata_minimization(dfa)\n",
    "display(mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Implemente un int\u00e9rprete para _expresiones regulares extendidas_ (operadores: `+`, `?`, `[ ]`, etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}