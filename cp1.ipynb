{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #1 (Compilación)\n",
    "\n",
    "A lo largo del curso estaremos implementando un compilador para el lenguaje de programación `HULK` (*Havana University Language for Kompilers*), paso a paso, introduciendo nuevas características del lenguaje o mejorando la implementación de otras características a medida que vamos descubriendo las técnicas fundamentales de la teoría de lenguajes y la compilación.\n",
    "\n",
    "El objetivo de esta clase es construir un evaluador de expresiones \"a mano\", usando los recursos que tenemos hasta el momento. Para ello vamos a comenzar con una versión de `HULK` muy sencilla, un lenguaje de expresiones aritméticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de expresiones\n",
    "\n",
    "Definiremos a continuación este lenguaje de manera informal:\n",
    "\n",
    "Un programa en `HULK` consta de una secuencia de expresiones. Cada expresión está compuesta por:\n",
    "\n",
    "- números (con coma flotante de 32 bits), \n",
    "- operadores `+ *` con el orden operacional, y\n",
    "- paréntesis `(` y `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis lexicográfico\n",
    "\n",
    "Comenzaremos construyendo un prototipo bien simple, donde asumiremos que en la expresión hay espacios en blanco entre todos los elementos, de modo que el *lexer* se reduce a dividir por espacios. Luego iremos adicionando elementos más complejos.\n",
    "\n",
    "El siguiente método devuelve una lista de *tokens*, asumiendo que la expresión solo tiene números, operadores y paréntesis, separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, '+', 6, '*', 9]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    \"\"\"     \n",
    "    return [int(item) if item.isnumeric() else item for item in text.split()]\n",
    "    \n",
    "assert tokenize('5 + 6 * 9') == [5, '+', 6, '*', 9]\n",
    "print(tokenize('5 + 6 * 9'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis sintáctico y evaluación\n",
    "\n",
    "Una vez que tenemos los *tokens*, solo nos queda evaluar la expresión. Usaremos para ello una idea simple, pero bien útil: evaluaremos recursivamente la expresión descendiendo por los distintos niveles de precedencia.\n",
    "\n",
    "Toda expresión del lenguaje puede ser vista como una suma de _términos_, donde cada uno de estos \"_términos_\" se descompone a su vez en operaciones de multiplicación entre _factores_. Incluso si no hay operadores `+` en toda la expresión queda claro que esta idea es válida puesto que estaríamos en presencia de una expresión formada por un solo _término_. Los _factores_ del lenguaje son todos unidades atómicas: por ahora solo números y expresiones complejas envueltas entre paréntesis. Nótese que el uso de paréntesis permite reiniciar el descenso por los niveles de precedencia (regresar a los niveles más altos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E -> T + T \n",
    "# T -> F * F\n",
    "# F -> num | (E)\n",
    "\n",
    "# E -> E + E | Term\n",
    "#Term -> Factor * Factor | Factor\n",
    "#Factor -> num|(Exp)\n",
    "\n",
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '*': lambda x,y: x * y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util classes and methods\n",
    "\n",
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Clase base para todas las excepciones del analisis sintactico(parsing)\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BadEOFError(ParsingError):\n",
    "\n",
    "    \"\"\"\n",
    "    Unexpected EOF error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        ParsingError.__init__(self, \"Unexpected EOF\")\n",
    "        \n",
    "class UnexpectedToken(ParsingError):\n",
    "    \n",
    "    \"\"\"\n",
    "    Error\n",
    "    Unexpected token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Unexpected token: {token} at {i}')\n",
    "        \n",
    "class MissingCloseParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "\n",
    "    Missing ')' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \")\" token at {i}. Got \"{token}\" instead')\n",
    "        \n",
    "class MissingOpenParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing '(' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \"(\" token at {i}. Got \"{token}\" instead')\n",
    "\n",
    "def get_token(tokens, i, error_type=BadEOFError):\n",
    "    \"\"\"\n",
    "    Retorna tokens[i] if 'i' esta en rango . si no lanza un ParsingError\n",
    "    Returns tokens[i] if 'i' is in range. Otherwise, raises ParsingError exception.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return tokens[i]\n",
    "    except IndexError:\n",
    "        raise error_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "#La siguiente gramatica es ambigua puesto que existen dos parse izq para una misma cadena \n",
    "# E -> E + E | E * E | num | (E)\n",
    "\n",
    "# Las precedencias de esta gramatica no es correcta\n",
    "# E -> T + T\n",
    "# T -> F * F\n",
    "# F -> num | (E)\n",
    "# La siguiente gramatica tiene las precedencias bien pero asocia mal\n",
    "# por ejemplo 1-1+1 = 1 sin embargo si asocia a la derecha entonces se hace 1-(1+1) != (1-1)+1\n",
    "# E -> T + E | T\n",
    "# T -> F * T | F\n",
    "# F -> int | (E)\n",
    "# La siguiente gramatica tiene las presedencias correctamente y asocia correctamante\n",
    "#  E -> E + T| T\n",
    "#  T -> T * F | F\n",
    "#  F -> int | (E)\n",
    "\n",
    "# Esta gramatica es correcta sin embargo crea un loop en el parser puesto que E -> E + T -> E + T + T ...\n",
    "# Entonce para un parser la recursividad izquierda es un problema\n",
    "# Como elimiarla\n",
    "# E-> TX\n",
    "# X -> +TX|epsilon\n",
    "# T -> FY\n",
    "# Y -> *FY|epsilon\n",
    "#  F -> int | (E)\n",
    "# Por ultimo queremos anadir funciones elementales con multiples argumentos.\n",
    "# E -> TX\n",
    "# X -> +TX |epsilon\n",
    "# T -> FY\n",
    "# Y -> *FY |epsilon\n",
    "# F -> int | (E) |srt(A)\n",
    "# A -> EC\n",
    "# C -> ,EC  | epsilon\n",
    "\n",
    "operations = {\n",
    "    '+': lambda x, y: x + y,\n",
    "    '-': lambda x, y: x - y,\n",
    "    '*': lambda x, y: x * y,\n",
    "    '/': lambda x, y: x / y,\n",
    "}\n",
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    Evaluates an expression recursively.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i, value = parse_expression(tokens, 0)\n",
    "        assert i == len(tokens)\n",
    "        return value\n",
    "    except ParsingError as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "\n",
    "expr_operations = ['+', '-']\n",
    "\n",
    "\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "    return parse_X(term, tokens, i)\n",
    "\n",
    "def parse_X(value, tokens, i):\n",
    "    if i < len(tokens):\n",
    "        token = get_token(tokens, i)\n",
    "        if token in expr_operations:\n",
    "            i, term = parse_term(tokens, i+1)\n",
    "            operation = operations[token]\n",
    "            value = operation(value, term)\n",
    "            i, value = parse_X(value, tokens, i)\n",
    "\n",
    "    return i, value\n",
    "\n",
    "\n",
    "term_operations = ['*', '/']\n",
    "\n",
    "\n",
    "def parse_term(tokens, i):\n",
    "    i, factor = parse_factor(tokens, i)\n",
    "    return parse_Y(factor, tokens, i)\n",
    "\n",
    "\n",
    "def parse_Y(value, tokens, i):\n",
    "    if i < len(tokens):\n",
    "        token = get_token(tokens, i)\n",
    "        if token in term_operations:\n",
    "            i, factor = parse_factor(tokens, i+1)\n",
    "            operation = operations[token]\n",
    "            value = operation(value, factor)\n",
    "            i, value = parse_Y(value, tokens, i)\n",
    "    return i, value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "    token = opert_fun =  get_token(tokens, i)\n",
    "\n",
    "    if opert_fun in functions.keys():\n",
    "        token = get_token(tokens, i+1)\n",
    "        if token == \"(\":\n",
    "            \n",
    "            i,args = parse_Arg(tokens,i+2)\n",
    "\n",
    "            try:\n",
    "                func = functions[opert_fun]\n",
    "                value = func(*args)\n",
    "            except:\n",
    "                raise UnexpectedToken(tokens[i],i)\n",
    "            \n",
    "            token = get_token(tokens, i)\n",
    "            if token != \")\":\n",
    "                raise MissingCloseParenthesisError(tokens[i], i)\n",
    "            return i+1,value\n",
    "\n",
    "        else:\n",
    "            raise MissingOpenParenthesisError(tokens[i], i)\n",
    "\n",
    "    if token == \"(\":\n",
    "        i, expression = parse_expression(tokens, i+1)\n",
    "        if get_token(tokens, i) != \")\":\n",
    "            raise MissingCloseParenthesisError(tokens, i)\n",
    "        return i+1, expression\n",
    "    else:\n",
    "        if isinstance(token, (int, float)):\n",
    "            return i+1, token\n",
    "        else:\n",
    "            raise UnexpectedToken(tokens[i], i)\n",
    "\n",
    "def parse_Arg(tokens, i):\n",
    "    args = []\n",
    "    i,value = parse_expression(tokens,i)\n",
    "    args.append(value)\n",
    "    i,args = parse_C(args,tokens,i)\n",
    "    return i,args\n",
    "    \n",
    "def parse_C(args,tokens,i):\n",
    "    if i < len(tokens):\n",
    "        token = get_token(tokens, i)\n",
    "        if token == \",\":\n",
    "            i, value = parse_expression(tokens, i+1)\n",
    "            args.append(value)\n",
    "            i,args  = parse_C(args,tokens, i)\n",
    "        return i,args\n",
    "\n",
    "\n",
    "\n",
    "print(evaluate(tokenize('5 + 6 * 9')))\n",
    "assert evaluate(tokenize('5 + 6 * 9')) == 59.\n",
    "assert evaluate(tokenize('( 5 + 6 ) * 9')) == 99.\n",
    "assert evaluate(tokenize('( 5 + 6 ) + 1 * 9 + 2')) == 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando constantes\n",
    "\n",
    "Agreguemos constantes numéricas al lenguaje `HULK` Para ello, simplemente añadiremos un diccionario con todas las constantes disponibles, que usaremos durante la tokenización. Nótese que solo es necesario modificar el _lexer_ para añadir este rasgo al lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'pi': 3.14159265359,\n",
    "    'e': 2.71828182846,\n",
    "    'phi': 1.61803398875,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    Replaces constants.\n",
    "    \"\"\"\n",
    "    return [int(item) if item.isnumeric()  else constants[item] if item in constants.keys() else item for item in expr.split()]\n",
    "\n",
    "assert tokenize('2 * pi') == [2.0, '*', 3.14159265359]\n",
    "\n",
    "tokenize('2 * pi') \n",
    "\n",
    "assert evaluate(tokenize('2 * pi')) == 6.28318530718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando operadores `-` y `/`\n",
    "\n",
    "- **Restricción:** No utilizar ciclos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '-': lambda x,y: x - y,\n",
    "    '*': lambda x,y: x * y,\n",
    "    '/': lambda x,y: x / y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_expression(tokens, i):\n",
    "#     # Insert your code here ...\n",
    "#     pass\n",
    "\n",
    "        \n",
    "# def parse_term(tokens, i):\n",
    "#     # Insert your code here ...\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate(tokenize('1 - 1 + 1')) == 1\n",
    "assert evaluate(tokenize('8 / 4 / 2')) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_.\n",
    "\n",
    "Para las funciones elementales haremos algo similar a las constantes, pero en vez de a la hora de tokenizar, las reemplazaremos a la hora de evaluar, pues necesitamos evaluar recursivamente los argumentos de la función. Empezaremos por garantizar que nuestro tokenizador que es capaz de reconocer expresiones con funciones elementales de más de un argumento, en caso de no ser así es necesario arreglarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize('log ( 64 , 1 + 3 )') == ['log', '(', 64.0, ',', 1.0, '+', 3.0, ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionaremos entonces un diccionario con todas las funciones elementales que permitiremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, modificaremos el método `evaluate` para que use las funciones elementales. Recordemos que los argumentos están separados por el token _coma_ (`,`) y que cada uno puede a su vez tener sub-expresiones que consistan también en llamados a funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_factor(tokens, i):\n",
    "#     # Insert your code here ...\n",
    "#     # (You may copy and modify your previous implementation of 'parse_factor')\n",
    "#     pass\n",
    "    \n",
    "assert evaluate(tokenize('log ( 64 , 1 + 3 )')) == 3.0"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
