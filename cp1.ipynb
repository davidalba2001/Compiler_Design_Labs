{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Pr\u00e1ctica #1 (Compilaci\u00f3n)\n",
    "\n",
    "A lo largo del curso estaremos implementando un compilador para el lenguaje de programaci\u00f3n `HULK` (*Havana University Language for Kompilers*), paso a paso, introduciendo nuevas caracter\u00edsticas del lenguaje o mejorando la implementaci\u00f3n de otras caracter\u00edsticas a medida que vamos descubriendo las t\u00e9cnicas fundamentales de la teor\u00eda de lenguajes y la compilaci\u00f3n.\n",
    "\n",
    "El objetivo de esta clase es construir un evaluador de expresiones \"a mano\", usando los recursos que tenemos hasta el momento. Para ello vamos a comenzar con una versi\u00f3n de `HULK` muy sencilla, un lenguaje de expresiones aritm\u00e9ticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de expresiones\n",
    "\n",
    "Definiremos a continuaci\u00f3n este lenguaje de manera informal:\n",
    "\n",
    "Un programa en `HULK` consta de una secuencia de expresiones. Cada expresi\u00f3n est\u00e1 compuesta por:\n",
    "\n",
    "- n\u00fameros (con coma flotante de 32 bits), \n",
    "- operadores `+ *` con el orden operacional, y\n",
    "- par\u00e9ntesis `(` y `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An\u00e1lisis lexicogr\u00e1fico\n",
    "\n",
    "Comenzaremos construyendo un prototipo bien simple, donde asumiremos que en la expresi\u00f3n hay espacios en blanco entre todos los elementos, de modo que el *lexer* se reduce a dividir por espacios. Luego iremos adicionando elementos m\u00e1s complejos.\n",
    "\n",
    "El siguiente m\u00e9todo devuelve una lista de *tokens*, asumiendo que la expresi\u00f3n solo tiene n\u00fameros, operadores y par\u00e9ntesis, separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for item in text.split():\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "assert tokenize('5 + 6 * 9') == [5, '+', 6, '*', 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An\u00e1lisis sint\u00e1ctico y evaluaci\u00f3n\n",
    "\n",
    "Una vez que tenemos los *tokens*, solo nos queda evaluar la expresi\u00f3n. Usaremos para ello una idea simple, pero bien \u00fatil: evaluaremos recursivamente la expresi\u00f3n descendiendo por los distintos niveles de precedencia.\n",
    "\n",
    "Toda expresi\u00f3n del lenguaje puede ser vista como una suma de _t\u00e9rminos_, donde cada uno de estos \"_t\u00e9rminos_\" se descompone a su vez en operaciones de multiplicaci\u00f3n entre _factores_. Incluso si no hay operadores `+` en toda la expresi\u00f3n queda claro que esta idea es v\u00e1lida puesto que estar\u00edamos en presencia de una expresi\u00f3n formada por un solo _t\u00e9rmino_. Los _factores_ del lenguaje son todos unidades at\u00f3micas: por ahora solo n\u00fameros y expresiones complejas envueltas entre par\u00e9ntesis. N\u00f3tese que el uso de par\u00e9ntesis permite reiniciar el descenso por los niveles de precedencia (regresar a los niveles m\u00e1s altos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '*': lambda x,y: x * y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util classes and methods\n",
    "\n",
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BadEOFError(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected EOF error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        ParsingError.__init__(self, \"Unexpected EOF\")\n",
    "        \n",
    "class UnexpectedToken(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Unexpected token: {token} at {i}')\n",
    "        \n",
    "class MissingCloseParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing ')' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \")\" token at {i}. Got \"{token}\" instead')\n",
    "        \n",
    "class MissingOpenParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing '(' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \"(\" token at {i}. Got \"{token}\" instead')\n",
    "\n",
    "def get_token(tokens, i, error_type=BadEOFError):\n",
    "    \"\"\"\n",
    "    Returns tokens[i] if 'i' is in range. Otherwise, raises ParsingError exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return tokens[i]\n",
    "    except IndexError:\n",
    "        raise error_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    Evaluates an expression recursively.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i, value = parse_expression(tokens, 0)\n",
    "        assert i == len(tokens)\n",
    "        return value\n",
    "    except ParsingError as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "    \n",
    "    if i < len(tokens):\n",
    "        if tokens[i] == '+':\n",
    "            # Insert your code here ...\n",
    "            pass\n",
    "    return i, term\n",
    "        \n",
    "def parse_term(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    pass\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    pass\n",
    "            \n",
    "assert evaluate(tokenize('5 + 6 * 9')) == 59.\n",
    "assert evaluate(tokenize('( 5 + 6 ) * 9')) == 99.\n",
    "assert evaluate(tokenize('( 5 + 6 ) + 1 * 9 + 2')) == 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando constantes\n",
    "\n",
    "Agreguemos constantes num\u00e9ricas al lenguaje `HULK` Para ello, simplemente a\u00f1adiremos un diccionario con todas las constantes disponibles, que usaremos durante la tokenizaci\u00f3n. N\u00f3tese que solo es necesario modificar el _lexer_ para a\u00f1adir este rasgo al lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'pi': 3.14159265359,\n",
    "    'e': 2.71828182846,\n",
    "    'phi': 1.61803398875,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    Replaces constants.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    for token in expr.split():\n",
    "        # Insert your code here ...\n",
    "        # (You may copy and modify your previous implementation of 'tokenize')\n",
    "        pass\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "assert tokenize('2 * pi') == [2.0, '*', 3.14159265359]\n",
    "assert evaluate(tokenize('2 * pi')) == 6.28318530718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando operadores `-` y `/`\n",
    "\n",
    "- **Restricci\u00f3n:** No utilizar ciclos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '-': lambda x,y: x - y,\n",
    "    '*': lambda x,y: x * y,\n",
    "    '/': lambda x,y: x / y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_expression(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    pass\n",
    "\n",
    "        \n",
    "def parse_term(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate(tokenize('1 - 1 + 1')) == 1\n",
    "assert evaluate(tokenize('8 / 4 / 2')) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se har\u00e1 en notaci\u00f3n prefija, comenzando por el nombre de la funci\u00f3n y seguido, entre par\u00e9ntesis, por los argumentos, que estar\u00e1n separados entre s\u00ed por _comas_.\n",
    "\n",
    "Para las funciones elementales haremos algo similar a las constantes, pero en vez de a la hora de tokenizar, las reemplazaremos a la hora de evaluar, pues necesitamos evaluar recursivamente los argumentos de la funci\u00f3n. Empezaremos por garantizar que nuestro tokenizador que es capaz de reconocer expresiones con funciones elementales de m\u00e1s de un argumento, en caso de no ser as\u00ed es necesario arreglarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize('log ( 64 , 1 + 3 )') == ['log', '(', 64.0, ',', 1.0, '+', 3.0, ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionaremos entonces un diccionario con todas las funciones elementales que permitiremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por \u00faltimo, modificaremos el m\u00e9todo `evaluate` para que use las funciones elementales. Recordemos que los argumentos est\u00e1n separados por el token _coma_ (`,`) y que cada uno puede a su vez tener sub-expresiones que consistan tambi\u00e9n en llamados a funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_factor(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    # (You may copy and modify your previous implementation of 'parse_factor')\n",
    "    pass\n",
    "    \n",
    "assert evaluate(tokenize('log ( 64 , 1 + 3 )')) == 3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}