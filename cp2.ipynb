{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #2 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un _parser_ para el subconjunto del lenguaje `HULK` descrito en la clase anterior. Esta vez nos apoyaremos en una descripción formal del lenguaje: una gramática libre del contexto.\n",
    "\n",
    "Recordemos que una gramática `G` es un cuádruplo `<T,N,S,P>` donde:\n",
    "- `T` es el conjunto de los _terminales_ (informalmente los símbolos que realmente se imprimiran en la cadena).\n",
    "- `N` es el conjunto de los _no terminales_ (símbolos intermedios usados al generar una cadena y que deberán ser reemplazados para obtener la cadena final).\n",
    "- `S` es _el símbolo distinguido_ de la gramática (por definición, toda cadena perteneciente al lenguaje generado por la gramática deriva en 0 o más pasos del símbolo distinguido, o sea, `L(G) = { w | S =>* w }` ).\n",
    "- `P` es el conjunto de las _producciones_ de la gramática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing recursivo descendente\n",
    "\n",
    "En conferencia se discutió la idea de construir un parser partiendo de la especificación de la gramática y usando una exploración con _backtrack_. Vimos que incluso algunas gramáticas podrían ser parseadas con este mecanismo sin hacer backtrack si quiera: el parser podría seleccionar de forma determinista qué producción aplicar para obtener la derivación de la cadena. A estas gramáticas les llamamos _gramáticas LL(1)_.\n",
    "\n",
    "A continuación se presenta una implementación base del mecanismo de parsing recursivo descendente. Este facilitará la construcción _\"ad hoc\"_ de parsers para gramáticas específicas. En clases posteriores estaremos automatizando la generación del parser a partir de la descripción de la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"\n",
    "    Basic token class. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lex : str\n",
    "        Token's lexeme.\n",
    "    token_type : Enum\n",
    "        Token's type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.lex}:{self.token_type}'\n",
    "        \n",
    "\n",
    "class Lexer:\n",
    "    \"\"\"\n",
    "    Base lexer class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String to tokenize.\n",
    "        Text is source program.\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        self.index = 0\n",
    "        self.text = text #source program\n",
    "        self.tokens = self.tokenize_text() \n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        \"\"\"\n",
    "        Tokenize `self.text` and set it to `self.tokens`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def next_token(self):\n",
    "        \"\"\"\n",
    "        Returns the next tokens readed by the lexer. `None` if `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            return token\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Returns whether or not `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.tokens[self.index]\n",
    "            return False\n",
    "        except IndexError:\n",
    "            return True\n",
    "            \n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    Base parser class.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lexer = None # intance lexer\n",
    "        self.left_parse = None #\n",
    "        self.lookahead = None #Point to the token-type to be parsed\n",
    "        \n",
    "    def build(self,lexer):\n",
    "        self.lexer = lexer\n",
    "        self.left_parse = []\n",
    "        self.lookahead = lexer.next_token().token_type\n",
    "        \n",
    "    def parse(self, lexer):\n",
    "        \"\"\"\n",
    "        Returns a left parse given the tokens from the lexer.\n",
    "        Retorna el left parse dado los tokens del lexer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.build(lexer)\n",
    "            self.begin()\n",
    "            return self.left_parse\n",
    "        \n",
    "        except ParsingError as error:\n",
    "            print(f'Parsing error: {error}!!!')\n",
    "            print(f'Lookahead: {self.lookahead}')\n",
    "            print(f'Unfinished parse: {self.left_parse}')\n",
    "            \n",
    "        finally:\n",
    "            self.lex = None\n",
    "            self.left_parse = None\n",
    "            self.lookahead = None\n",
    "            \n",
    "            \n",
    "    def begin(self):\n",
    "        \"\"\"\n",
    "        Begin parsing from starting symbol and match EOF.\n",
    "        Inicia el parseo en el simbolo incial de la gramatica y chequea \n",
    "        si se consumio toda la cadena o sea que el ultimo toquen machea con EOF\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def report(self, production):\n",
    "        \"\"\"\n",
    "        Adds production to the left parse that is being build.\n",
    "        Anade producciones al left parse \n",
    "        \"\"\"\n",
    "        self.left_parse.append(production)\n",
    "        \n",
    "    def error(self, msg=None):\n",
    "        \"\"\"\n",
    "        Raises a parsing error.\n",
    "        Eleva un error de parseo\n",
    "        \"\"\"\n",
    "        raise ParsingError(msg)\n",
    "        \n",
    "    def match(self, token_type):\n",
    "        \"\"\"\n",
    "        Consumes one token from the lexer if lookahead matches the given token type.\n",
    "        Raises parsing error otherwise.\n",
    "        Si el token en lookahead machea en tipo con token_type entonces consume ese \n",
    "        token so no eleva un error.\n",
    "        \"\"\"\n",
    "        if token_type == self.lookahead:\n",
    "            try:\n",
    "                self.lookahead = self.lexer.next_token().token_type\n",
    "            except AttributeError:\n",
    "                self.lookahead = None\n",
    "        else:\n",
    "            self.error('Unexpected token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HULK\n",
    "\n",
    "Comenzaremos la construcción del parser para `HULK` definiendo los tokens del lenguaje. Estos tokens representan a su vez los símbolos terminales de la gramática con la que trabajará el parser. Se incluye un token `EOF` usado para marcar el fin la cadena. En `fixed_tokens` se almacenan los tokens con lexemas constantes para simplificar la implementación del parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "TokenType = Enum('TokenType', 'eof num plus minus star div opar cpar comma id')\n",
    "\n",
    "EOF_TOKEN = Token('$', TokenType.eof)\n",
    "\n",
    "def is_valid_name(name):\n",
    "    # Check if the name is not empty and starts with a letter or underscore\n",
    "    if name and (name[0].isalpha() or name[0] == '_'):\n",
    "        # Check if all characters are alphanumeric or underscores\n",
    "        if all(char.isalnum() or char == '_' for char in name):\n",
    "            # Check if it's not a Python built-in function\n",
    "            if name not in dir(__builtins__):\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "def get_id_token(name):\n",
    "    if(is_valid_name(name)):\n",
    "        return Token( name, TokenType.id)\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "fixed_tokens = {\n",
    "    '+':   Token('+', TokenType.plus),\n",
    "    '-':   Token('-', TokenType.minus),\n",
    "    '*':   Token('*', TokenType.star),\n",
    "    '/':   Token('/', TokenType.div),\n",
    "    '(':   Token('(', TokenType.opar),\n",
    "    ')':   Token(')', TokenType.cpar),\n",
    "    ',':   Token(',', TokenType.comma),\n",
    "    'pi':   Token(3.14159265359, TokenType.num),\n",
    "    'e':   Token(2.71828182846, TokenType.num),\n",
    "    'phi':   Token(1.61803398875, TokenType.num),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "La implementación del lexer es muy similar al de la clase anterior. Por ahora asumiremos que los lexemas relevantes están separados por espacios, por lo que el lexer simplemente debería separar por espacios la cadena de entrada y construir los tokens correspondientes. El lexer deberá incluir un token EOF al final de la secuencia de tokens. Esto resulta conveniente durante el proceso de parsing para evitar manejar el fin de la cadena como un caso extremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_number(number):\n",
    "    if number.isnumeric():  # Check if it's an integer\n",
    "        return int\n",
    "    elif \".\" in number and number.replace(\".\", \"\", 1).isnumeric():  # Check if it's a float\n",
    "        return float\n",
    "    else:\n",
    "        return \"Not a valid number\"\n",
    "\n",
    "def check_is_number(number):\n",
    "    return check_type_number(number) in (int,float)\n",
    "\n",
    "def get_numToken(number:str):\n",
    "        type_token = check_type_number(number)\n",
    "        if type_token in (int,float):\n",
    "            return Token(type_token(number), TokenType.num)\n",
    "        \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKLexer(Lexer):\n",
    "            \n",
    "    def tokenize_text(self):\n",
    "        tokens = []\n",
    "        text = self.text\n",
    "        \n",
    "        tokens = [get_numToken(item) if check_is_number(item) else fixed_tokens[item] if item in fixed_tokens.keys() else get_id_token(item) if is_valid_name(item)  else Exception(f\"Tokenized error: token {item} not in grammar.\") for item in text.split()]\n",
    "\n",
    "        error_tokens = [token for token in tokens if isinstance(token, Exception)]\n",
    "        if error_tokens:\n",
    "            raise Exception(\"\\n\".join(str(token) for token in error_tokens))\n",
    "        \n",
    "        tokens.append(EOF_TOKEN)\n",
    "        \n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser de HULK\n",
    "\n",
    "Podemos intuir la siguiente gramática de `HULK` a partir de las consideraciones realizadas en la clase anterior:\n",
    "``` \n",
    "E --> T + E | T\n",
    "T --> F * T | F\n",
    "F --> ( E ) | n\n",
    "``` \n",
    "\n",
    "Sin embargo, rápidamente podemos notar que un parser recursivo descendente deberá dar \"un salto de fe\" para decidir qué producción, entre `E --> T + E` y `E --> T`, aplicar desde el inicio. Claro está que como ambos comienzan con `T` se puede aplazar la decisión de cuál producción aplicar hasta terminar de consumir `T`. Para evitar enfrentarnos a esto realizaremos una modificación a la gramática conocida como _eliminación de prefijos comunes_. Para ello, todas las producciones con la misma cabecera que comiencen con el mismo símbolo serán modificadas, con lo cual obtenemos:\n",
    "\n",
    "```\n",
    "E --> T X\n",
    "X --> + T X | - T X | epsilon\n",
    "T --> F Y\n",
    "Y --> * F Y | / F Y | epsilon\n",
    "F --> ( E ) | n\n",
    "```\n",
    "\n",
    "Como podremos comprobar con la implementación del parser de `HULK` según la gramática anterior, dicha gramática es _LL(1)_. En todo momento sabremos qué producción aplicar con solo ver el símbolo actual de la cadena.\n",
    "\n",
    "Para construir el parser según la gramática anterior simplemente extenderemos la clase `Parser` (usando herencia) para incluir un método por cada no terminal de la gramática. En estos métodos deberemos explorar las posibles producciones a aplicar en función del símbolo actual de la cadena (`lookahead`). Según la rama que se decida seguir, invocaremos los métodos correspondientes a los no terminales que aparezcan en la parte derecha de la producción aplicada. Por cada terminal que aparezca en la parte derecha haremos una invocación al método `match` con el tipo del token correspondiente. Este procedimiento se realiza en el orden en que aparezcan los símbolos en la producción. \n",
    "\n",
    "> **OJO:** el caso de las producciones con la forma `X --> epsilon` puede ser un tanto especial de seleccionar. Intente descubrir un forma para seleccionar dichas ramas.\n",
    "\n",
    "Es importante garantizar una invocación al método `error` en caso que ninguna de las producciones (ramas) deba ser aplicada. Esto puede saberse con un análisis manual sobre la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HULKParser(Parser):\n",
    "\n",
    "    def begin(self):\n",
    "        self.E()\n",
    "        self.match(TokenType.eof)\n",
    "\n",
    "    def E(self):\n",
    "        \"\"\"\n",
    "        E --> TX\n",
    "        \"\"\"\n",
    "        if self.lookahead in (TokenType.num, TokenType.opar):\n",
    "            self.report('E --> TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        else:\n",
    "            self.error('Malformed expression')\n",
    "\n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        X --> +TX | -TX | epsilon\n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.plus:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('X --> +TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        elif self.lookahead is TokenType.minus:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('X --> -TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        else:\n",
    "            self.report('X --> epsilon')\n",
    "\n",
    "    def T(self):\n",
    "        \"\"\"\n",
    "        T --> FY\n",
    "        \"\"\"\n",
    "        self.report('T --> FY')\n",
    "        self.F()\n",
    "        self.Y()\n",
    "\n",
    "    def Y(self):\n",
    "        \"\"\"\n",
    "        Y --> *FY | /FY | epsilon\n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.star:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> *FY')\n",
    "            self.F()\n",
    "            self.Y()\n",
    "\n",
    "        elif self.lookahead is TokenType.div:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> /FY')\n",
    "            self.F()\n",
    "            self.Y()\n",
    "\n",
    "        else:\n",
    "            self.report('Y --> epsilon')\n",
    "\n",
    "    def F(self):\n",
    "        \"\"\"\n",
    "        F --> n | (E) \n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.num:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('F --> n')\n",
    "\n",
    "        elif self.lookahead is TokenType.opar:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> (E)')\n",
    "            self.E()\n",
    "            if self.lookahead is TokenType.cpar:\n",
    "                self.match(self.lookahead)\n",
    "            else:\n",
    "                self.error('Unexpected token')\n",
    "        else:\n",
    "            self.error('Unexpected token')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Cerraremos el _pipeline_ conectando el lexer y el parser, siendo el primero el encargado de preprocesar la cadena de entrada. El parser devuelve un parse izquierdo, con el cual seremos capacez de reconstruir el árbol de derivación y posteriormente evaluar la expresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_parse(text):\n",
    "    lexer = HULKLexer(text)\n",
    "    parser = HULKParser()\n",
    "    return parser.parse(lexer)\n",
    "\n",
    "\n",
    "\n",
    "assert get_left_parse('5 + 8 * 9') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> *FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]\n",
    "\n",
    "assert get_left_parse('1 - 1 + 1') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> -TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifcando la Gramatica para Aceptar Expresiones Funcionales\n",
    "\n",
    "```\n",
    "E --> T X\n",
    "X --> + T X | - T X | epsilon\n",
    "T --> F Y\n",
    "Y --> * F Y | / F Y | epsilon\n",
    "F --> ( E ) | n | id(A)\n",
    "A --> EC | epsilon\n",
    "C --> ,EC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKParser(Parser):\n",
    "\n",
    "    def begin(self):\n",
    "        self.E()\n",
    "        self.match(TokenType.eof)\n",
    "\n",
    "    def E(self):\n",
    "        \"\"\"\n",
    "        E --> TX\n",
    "        \"\"\"\n",
    "        if self.lookahead in (TokenType.num, TokenType.id, TokenType.opar):\n",
    "            self.report('E --> TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        else:\n",
    "            self.error(\n",
    "                f\"Malformed expression. Found: {self.lookahead}. Expecting a number, identifier, or operator.\")\n",
    "\n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        X --> +TX | -TX | epsilon\n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.plus:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('X --> +TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        elif self.lookahead is TokenType.minus:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('X --> -TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "\n",
    "        elif self.lookahead in (TokenType.eof,TokenType.comma,TokenType.cpar):\n",
    "            self.report('X --> epsilon')\n",
    "\n",
    "        else:\n",
    "            self.error(\n",
    "                f\"Unexpected token. Found: {self.lookahead}. Expecting '+', '-',',' end of file, or ')'.\")\n",
    "\n",
    "    def T(self):\n",
    "        \"\"\"\n",
    "        T --> FY\n",
    "        \"\"\"\n",
    "        if self.lookahead in (TokenType.num, TokenType.id, TokenType.opar):\n",
    "            self.report('T --> FY')\n",
    "            self.F()\n",
    "            self.Y()\n",
    "        else:\n",
    "            self.error(\n",
    "                f\"Malformed expression. Found: {self.lookahead}. Expecting a number, identifier, or operator.\")\n",
    "\n",
    "    def Y(self):\n",
    "        \"\"\"\n",
    "        Y --> *FY | /FY | epsilon\n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.star:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> *FY')\n",
    "            self.F()\n",
    "            self.Y()\n",
    "\n",
    "        elif self.lookahead is TokenType.div:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> /FY')\n",
    "            self.F()\n",
    "            self.Y()\n",
    "\n",
    "        elif self.lookahead in (TokenType.plus,TokenType.minus,TokenType.comma,TokenType.eof, TokenType.cpar):\n",
    "            self.report('Y --> epsilon')\n",
    "        \n",
    "        else:\n",
    "            self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting '*', '/','+','-', end of file, or ')'.\")\n",
    "\n",
    "    def F(self):\n",
    "        \"\"\"\n",
    "        F --> ( E ) | n | id(A)\n",
    "        \"\"\"\n",
    "        if self.lookahead is TokenType.num:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('F --> n')\n",
    "\n",
    "        elif self.lookahead is TokenType.opar:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('F --> (E)')\n",
    "            self.E()\n",
    "            if self.lookahead is TokenType.cpar:\n",
    "                self.match(self.lookahead)\n",
    "            else:\n",
    "                self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting ')' after expression\")\n",
    "\n",
    "        elif self.lookahead is TokenType.id:\n",
    "            self.match(self.lookahead)\n",
    "            self.report('Y --> id(A)')\n",
    "\n",
    "            if self.lookahead is TokenType.opar:\n",
    "                self.match(self.lookahead)\n",
    "                self.A()\n",
    "                \n",
    "                if self.lookahead is TokenType.cpar:\n",
    "                    self.match(self.lookahead)\n",
    "                else:\n",
    "                    self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting ')' after arguments\")\n",
    "            else:\n",
    "                self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting '(' after function identifier\")\n",
    "        else:\n",
    "            self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting '(', number, or identifier.\")\n",
    "\n",
    "\n",
    "    def A(self):\n",
    "        \"A --> EC | epsilon\"\n",
    "        if self.lookahead is not TokenType.cpar:\n",
    "            self.report(\"A --> EC\")\n",
    "            self.E()\n",
    "            self.C()\n",
    "\n",
    "        elif self.lookahead is TokenType.cpar:\n",
    "            self.report(\"A --> epsilon\")\n",
    "\n",
    "        else:\n",
    "            self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting ')' after arguments\")\n",
    "        \n",
    "\n",
    "    def C(self):\n",
    "        \"C --> ,EC | epsilon\"\n",
    "        if self.lookahead is TokenType.comma:\n",
    "            self.report(\"C --> ,EC\")\n",
    "            self.match(self.lookahead)\n",
    "            self.E()\n",
    "            self.C()\n",
    "        elif self.lookahead is TokenType.cpar:\n",
    "            self.report(\"C --> epsilon\")\n",
    "\n",
    "        else:\n",
    "            self.error(f\"Unexpected token. Found: {self.lookahead}. Expecting ',' or ')'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenType.num\n"
     ]
    }
   ],
   "source": [
    "print(TokenType.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E --> TX',\n",
       " 'T --> FY',\n",
       " 'Y --> id(A)',\n",
       " 'A --> EC',\n",
       " 'E --> TX',\n",
       " 'T --> FY',\n",
       " 'F --> n',\n",
       " 'Y --> epsilon',\n",
       " 'X --> epsilon',\n",
       " 'C --> ,EC',\n",
       " 'E --> TX',\n",
       " 'T --> FY',\n",
       " 'F --> n',\n",
       " 'Y --> epsilon',\n",
       " 'X --> epsilon',\n",
       " 'C --> epsilon',\n",
       " 'Y --> epsilon',\n",
       " 'X --> epsilon']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_left_parse(text):\n",
    "    lexer = HULKLexer(text)\n",
    "    parser = HULKParser()\n",
    "    return parser.parse(lexer)\n",
    "\n",
    "get_left_parse('log ( 2 , 2 )')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucción del árbol de derivación y evaluación\n",
    "\n",
    "- Realice las modificaciones pertinentes para que las producciones reportadas por el parser nos permitan reconstruir el árbol de derivación. Note que la implementación actual trabaja con `str` y se desechan los tokens (que son los contenedores de los lexemas).\n",
    "- Utilice el parse izquierdo y/o árbol de derivación para evaluar la expresión. Note que la estructura de la gramática causa que los operadores (+, -, \\*, /) asocien hacia la derecha, lo cual conlleva problemas si se evalúa recursivamente sin considerar tal característica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
