{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Pr\u00e1ctica #2 (Compilaci\u00f3n)\n",
    "\n",
    "En esta clase estaremos implementando un _parser_ para el subconjunto del lenguaje `HULK` descrito en la clase anterior. Esta vez nos apoyaremos en una descripci\u00f3n formal del lenguaje: una gram\u00e1tica libre del contexto.\n",
    "\n",
    "Recordemos que una gram\u00e1tica `G` es un cu\u00e1druplo `<T,N,S,P>` donde:\n",
    "- `T` es el conjunto de los _terminales_ (informalmente los s\u00edmbolos que realmente se imprimiran en la cadena).\n",
    "- `N` es el conjunto de los _no terminales_ (s\u00edmbolos intermedios usados al generar una cadena y que deber\u00e1n ser reemplazados para obtener la cadena final).\n",
    "- `S` es _el s\u00edmbolo distinguido_ de la gram\u00e1tica (por definici\u00f3n, toda cadena perteneciente al lenguaje generado por la gram\u00e1tica deriva en 0 o m\u00e1s pasos del s\u00edmbolo distinguido, o sea, `L(G) = { w | S =>* w }` ).\n",
    "- `P` es el conjunto de las _producciones_ de la gram\u00e1tica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing recursivo descendente\n",
    "\n",
    "En conferencia se discuti\u00f3 la idea de construir un parser partiendo de la especificaci\u00f3n de la gram\u00e1tica y usando una exploraci\u00f3n con _backtrack_. Vimos que incluso algunas gram\u00e1ticas podr\u00edan ser parseadas con este mecanismo sin hacer backtrack si quiera: el parser podr\u00eda seleccionar de forma determinista qu\u00e9 producci\u00f3n aplicar para obtener la derivaci\u00f3n de la cadena. A estas gram\u00e1ticas les llamamos _gram\u00e1ticas LL(1)_.\n",
    "\n",
    "A continuaci\u00f3n se presenta una implementaci\u00f3n base del mecanismo de parsing recursivo descendente. Este facilitar\u00e1 la construcci\u00f3n _\"ad hoc\"_ de parsers para gram\u00e1ticas espec\u00edficas. En clases posteriores estaremos automatizando la generaci\u00f3n del parser a partir de la descripci\u00f3n de la gram\u00e1tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"\n",
    "    Basic token class. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lex : str\n",
    "        Token's lexeme.\n",
    "    token_type : Enum\n",
    "        Token's type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "        \n",
    "\n",
    "class Lexer:\n",
    "    \"\"\"\n",
    "    Base lexer class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String to tokenize.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.index = 0\n",
    "        self.text = text\n",
    "        self.tokens = self.tokenize_text()\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        \"\"\"\n",
    "        Tokenize `self.text` and set it to `self.tokens`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def next_token(self):\n",
    "        \"\"\"\n",
    "        Returns the next tokens readed by the lexer. `None` if `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            return token\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Returns whether or not `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.tokens[self.index]\n",
    "            return False\n",
    "        except IndexError:\n",
    "            return True\n",
    "            \n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    Base parser class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexer = None\n",
    "        self.left_parse = None\n",
    "        self.lookahead = None\n",
    "        \n",
    "    def parse(self, lexer):\n",
    "        \"\"\"\n",
    "        Returns a left parse given the tokens from the lexer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.lexer = lexer\n",
    "            self.left_parse = []\n",
    "            self.lookahead = lexer.next_token().token_type\n",
    "            self.begin()\n",
    "            return self.left_parse\n",
    "        \n",
    "        except ParsingError as error:\n",
    "            print(f'Parsing error: {error}!!!')\n",
    "            print(f'Lookahead: {self.lookahead}')\n",
    "            print(f'Unfinished parse: {self.left_parse}')\n",
    "            \n",
    "        finally:\n",
    "            self.lex = None\n",
    "            self.left_parse = None\n",
    "            self.lookahead = None\n",
    "            \n",
    "    def begin(self):\n",
    "        \"\"\"\n",
    "        Begin parsing from starting symbol and match EOF.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def report(self, production):\n",
    "        \"\"\"\n",
    "        Adds production to the left parse that is being build.\n",
    "        \"\"\"\n",
    "        self.left_parse.append(production)\n",
    "        \n",
    "    def error(self, msg=None):\n",
    "        \"\"\"\n",
    "        Raises a parsing error.\n",
    "        \"\"\"\n",
    "        raise ParsingError(msg)\n",
    "        \n",
    "    def match(self, token_type):\n",
    "        \"\"\"\n",
    "        Consumes one token from the lexer if lookahead matches the given token type.\n",
    "        Raises parsing error otherwise.\n",
    "        \"\"\"\n",
    "        if token_type == self.lookahead:\n",
    "            try:\n",
    "                self.lookahead = self.lexer.next_token().token_type\n",
    "            except AttributeError:\n",
    "                self.lookahead = None\n",
    "        else:\n",
    "            self.error('Unexpected token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HULK\n",
    "\n",
    "Comenzaremos la construcci\u00f3n del parser para `HULK` definiendo los tokens del lenguaje. Estos tokens representan a su vez los s\u00edmbolos terminales de la gram\u00e1tica con la que trabajar\u00e1 el parser. Se incluye un token `EOF` usado para marcar el fin la cadena. En `fixed_tokens` se almacenan los tokens con lexemas constantes para simplificar la implementaci\u00f3n del parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "TokenType = Enum('TokenType', 'eof num plus minus star div opar cpar id')\n",
    "\n",
    "EOF_TOKEN = Token('$', TokenType.eof)\n",
    "\n",
    "fixed_tokens = {\n",
    "    '+'  :   Token( '+'           , TokenType.plus  ),\n",
    "    '-'  :   Token( '-'           , TokenType.minus ),\n",
    "    '*'  :   Token( '*'           , TokenType.star  ),\n",
    "    '/'  :   Token( '/'           , TokenType.div   ),\n",
    "    '('  :   Token( '('           , TokenType.opar  ),\n",
    "    ')'  :   Token( ')'           , TokenType.cpar  ),\n",
    "    'pi' :   Token( 3.14159265359 , TokenType.num   ),\n",
    "    'e'  :   Token( 2.71828182846 , TokenType.num   ),\n",
    "    'phi':   Token( 1.61803398875 , TokenType.num   ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "La implementaci\u00f3n del lexer es muy similar al de la clase anterior. Por ahora asumiremos que los lexemas relevantes est\u00e1n separados por espacios, por lo que el lexer simplemente deber\u00eda separar por espacios la cadena de entrada y construir los tokens correspondientes. El lexer deber\u00e1 incluir un token EOF al final de la secuencia de tokens. Esto resulta conveniente durante el proceso de parsing para evitar manejar el fin de la cadena como un caso extremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKLexer(Lexer):\n",
    "\n",
    "    def tokenize_text(self):\n",
    "        tokens = []\n",
    "        text = self.text\n",
    "        \n",
    "        for item in text.split():\n",
    "            # Insert your code here ...\n",
    "            pass\n",
    "            \n",
    "        # Is something missing?\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser de HULK\n",
    "\n",
    "Podemos intuir la siguiente gram\u00e1tica de `HULK` a partir de las consideraciones realizadas en la clase anterior:\n",
    "``` \n",
    "E --> T + E | T\n",
    "T --> F * T | F\n",
    "F --> ( E ) | n\n",
    "``` \n",
    "\n",
    "Sin embargo, r\u00e1pidamente podemos notar que un parser recursivo descendente deber\u00e1 dar \"un salto de fe\" para decidir qu\u00e9 producci\u00f3n, entre `E --> T + E` y `E --> T`, aplicar desde el inicio. Claro est\u00e1 que como ambos comienzan con `T` se puede aplazar la decisi\u00f3n de cu\u00e1l producci\u00f3n aplicar hasta terminar de consumir `T`. Para evitar enfrentarnos a esto realizaremos una modificaci\u00f3n a la gram\u00e1tica conocida como _eliminaci\u00f3n de prefijos comunes_. Para ello, todas las producciones con la misma cabecera que comiencen con el mismo s\u00edmbolo ser\u00e1n modificadas, con lo cual obtenemos:\n",
    "\n",
    "```\n",
    "E --> T X\n",
    "X --> + T X | - T X | epsilon\n",
    "T --> F Y\n",
    "Y --> * F Y | / F Y | epsilon\n",
    "F --> ( E ) | n\n",
    "```\n",
    "\n",
    "Como podremos comprobar con la implementaci\u00f3n del parser de `HULK` seg\u00fan la gram\u00e1tica anterior, dicha gram\u00e1tica es _LL(1)_. En todo momento sabremos qu\u00e9 producci\u00f3n aplicar con solo ver el s\u00edmbolo actual de la cadena.\n",
    "\n",
    "Para construir el parser seg\u00fan la gram\u00e1tica anterior simplemente extenderemos la clase `Parser` (usando herencia) para incluir un m\u00e9todo por cada no terminal de la gram\u00e1tica. En estos m\u00e9todos deberemos explorar las posibles producciones a aplicar en funci\u00f3n del s\u00edmbolo actual de la cadena (`lookahead`). Seg\u00fan la rama que se decida seguir, invocaremos los m\u00e9todos correspondientes a los no terminales que aparezcan en la parte derecha de la producci\u00f3n aplicada. Por cada terminal que aparezca en la parte derecha haremos una invocaci\u00f3n al m\u00e9todo `match` con el tipo del token correspondiente. Este procedimiento se realiza en el orden en que aparezcan los s\u00edmbolos en la producci\u00f3n. \n",
    "\n",
    "> **OJO:** el caso de las producciones con la forma `X --> epsilon` puede ser un tanto especial de seleccionar. Intente descubrir un forma para seleccionar dichas ramas.\n",
    "\n",
    "Es importante garantizar una invocaci\u00f3n al m\u00e9todo `error` en caso que ninguna de las producciones (ramas) deba ser aplicada. Esto puede saberse con un an\u00e1lisis manual sobre la gram\u00e1tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKParser(Parser):\n",
    "    def begin(self):\n",
    "        self.E()\n",
    "        self.match(TokenType.eof)\n",
    "        \n",
    "    def E(self):\n",
    "        \"\"\"\n",
    "        E --> TX\n",
    "        \"\"\"\n",
    "        if self.lookahead in (TokenType.num, TokenType.opar):\n",
    "            self.report('E --> TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "            \n",
    "        else:\n",
    "            self.error('Malformed expression')\n",
    "        \n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        X --> +TX | -TX | epsilon\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "        \n",
    "    def T(self):\n",
    "        \"\"\"\n",
    "        T --> FY\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "            \n",
    "    def Y(self):\n",
    "        \"\"\"\n",
    "        Y --> *FY | /FY | epsilon\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "            \n",
    "    def F(self):\n",
    "        \"\"\"\n",
    "        F --> n | (E)\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Cerraremos el _pipeline_ conectando el lexer y el parser, siendo el primero el encargado de preprocesar la cadena de entrada. El parser devuelve un parse izquierdo, con el cual seremos capacez de reconstruir el \u00e1rbol de derivaci\u00f3n y posteriormente evaluar la expresi\u00f3n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_parse(text):\n",
    "    lexer = HULKLexer(text)\n",
    "    parser = HULKParser()\n",
    "    return parser.parse(lexer)\n",
    "\n",
    "assert get_left_parse('5 + 8 * 9') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> *FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]\n",
    "\n",
    "assert get_left_parse('1 - 1 + 1') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> -TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se har\u00e1 en notaci\u00f3n prefija, comenzando por el nombre de la funci\u00f3n y seguido, entre par\u00e9ntesis, por los argumentos, que estar\u00e1n separados entre s\u00ed por _comas_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucci\u00f3n del \u00e1rbol de derivaci\u00f3n y evaluaci\u00f3n\n",
    "\n",
    "- Realice las modificaciones pertinentes para que las producciones reportadas por el parser nos permitan reconstruir el \u00e1rbol de derivaci\u00f3n. Note que la implementaci\u00f3n actual trabaja con `str` y se desechan los tokens (que son los contenedores de los lexemas).\n",
    "- Utilice el parse izquierdo y/o \u00e1rbol de derivaci\u00f3n para evaluar la expresi\u00f3n. Note que la estructura de la gram\u00e1tica causa que los operadores (+, -, \\*, /) asocien hacia la derecha, lo cual conlleva problemas si se eval\u00faa recursivamente sin considerar tal caracter\u00edstica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}